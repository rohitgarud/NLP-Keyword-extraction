{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>keywords</th>\n",
       "      <th>abstract</th>\n",
       "      <th>session</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ensemble Statistical and Heuristic Models for ...</td>\n",
       "      <td>statistical word alignment, ensemble learning,...</td>\n",
       "      <td>Statistical word alignment models need large a...</td>\n",
       "      <td>Ensemble Methods</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Improving Spectral Learning by Using Multiple ...</td>\n",
       "      <td>representation, spectral learning, discrete fo...</td>\n",
       "      <td>Spectral learning algorithms learn an unknown ...</td>\n",
       "      <td>Ensemble Methods</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Applying Swarm Ensemble Clustering Technique f...</td>\n",
       "      <td>software defect prediction, particle swarm opt...</td>\n",
       "      <td>Number of defects remaining in a system provid...</td>\n",
       "      <td>Ensemble Methods</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reducing the Effects of Detrimental Instances</td>\n",
       "      <td>filtering, label noise, instance weighting</td>\n",
       "      <td>Not all instances in a data set are equally be...</td>\n",
       "      <td>Ensemble Methods</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Concept Drift Awareness in Twitter Streams</td>\n",
       "      <td>twitter, adaptation models, time-frequency ana...</td>\n",
       "      <td>Learning in non-stationary environments is not...</td>\n",
       "      <td>Ensemble Methods</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      title  \\\n",
       "paper_id                                                      \n",
       "1         Ensemble Statistical and Heuristic Models for ...   \n",
       "2         Improving Spectral Learning by Using Multiple ...   \n",
       "3         Applying Swarm Ensemble Clustering Technique f...   \n",
       "4             Reducing the Effects of Detrimental Instances   \n",
       "5                Concept Drift Awareness in Twitter Streams   \n",
       "\n",
       "                                                   keywords  \\\n",
       "paper_id                                                      \n",
       "1         statistical word alignment, ensemble learning,...   \n",
       "2         representation, spectral learning, discrete fo...   \n",
       "3         software defect prediction, particle swarm opt...   \n",
       "4                filtering, label noise, instance weighting   \n",
       "5         twitter, adaptation models, time-frequency ana...   \n",
       "\n",
       "                                                   abstract           session  \\\n",
       "paper_id                                                                        \n",
       "1         Statistical word alignment models need large a...  Ensemble Methods   \n",
       "2         Spectral learning algorithms learn an unknown ...  Ensemble Methods   \n",
       "3         Number of defects remaining in a system provid...  Ensemble Methods   \n",
       "4         Not all instances in a data set are equally be...  Ensemble Methods   \n",
       "5         Learning in non-stationary environments is not...  Ensemble Methods   \n",
       "\n",
       "          year  \n",
       "paper_id        \n",
       "1         2014  \n",
       "2         2014  \n",
       "3         2014  \n",
       "4         2014  \n",
       "5         2014  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the data \n",
    "dataset_csv = \"ICMLA_2014_2015_2016_2017.csv\"\n",
    "encoding = \"ISO-8859-1\"\n",
    "data_df = pd.read_csv(dataset_csv, encoding=encoding).set_index(\"paper_id\")\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(448, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Preprocessing Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reducing the Effects of Detrimental Instances Not all instances in a data set are equally beneficial for inducing a model of the data. Some instances (such as outliers or noise) can be detrimental. However, at least initially, the instances in a data set are generally considered equally in machine learning algorithms. Many current approaches for handling noisy and detrimental instances make a binary decision about whether an instance is detrimental or not. In this paper, we 1) extend this paradigm by weighting the instances on a continuous scale and 2) present a methodology for measuring how detrimental an instance may be for inducing a model of the data. We call our method of identifying and weighting detrimental instances reduced detrimental instance learning (RDIL). We examine RIDL on a set of 54 data sets and 5 learning algorithms and compare RIDL with other weighting and filtering approaches. RDIL is especially useful for learning algorithms where every instance can affect the classification boundary and the training instances are considered individually, such as multilayer perceptrons trained with backpropagation (MLPs). Our results also suggest that a more accurate estimate of which instances are detrimental can have a significant positive impact for handling them.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[\"text\"] = data_df[\"title\"] + \" \" + data_df[\"abstract\"]\n",
    "data_df[\"text\"].values[3] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abbreviation Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RDIL': 'reduced detrimental instance learning'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['RDIL'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modified from https://stackoverflow.com/questions/63631451/how-to-match-abbreviations-with-their-meaning-with-regex\n",
    "import re\n",
    "def contains_abbrev(abbrev, text):\n",
    "    text = text.lower()\n",
    "    if not abbrev.isupper():\n",
    "        return False\n",
    "    cnt = 0\n",
    "    for c in abbrev.lower():\n",
    "        if text.find(c) > -1:\n",
    "            text = text[text.find(c):]\n",
    "            cnt += 1\n",
    "    return cnt == len(abbrev)\n",
    "\n",
    "# text= \"Some example text (SET) that demonstrates what I'm looking for. Energy system models (ESM) are used to find specific optima (SCO). Some say computer systems (CUST) are cool. In the summer playing outside (OUTS) should be preferred. Stupid example(s) Stupid example(S) Not stupid example (NSEMPLE), bad example (Bexle)\"\n",
    "text = data_df[\"text\"].values[3]\n",
    "pattern = r'\\b(([A-Z])\\w*(?:\\s+\\w+)*?)\\s*\\((\\2[A-Z]*)\\)'\n",
    "abbreviations = {x.group(3):x.group(1) for x in re.finditer(pattern, text, re.I) if contains_abbrev(x.group(3), x.group(1))}\n",
    "print(abbreviations)\n",
    "abbreviations.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to replace all the abbreviations from individual document using abbreviation definitions from that document. We have to remove the first occurance of the abbreviation from the text as it is from the abbreviation definition and replace other occurances with the expanded version (definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing the Effects of Detrimental Instances Not all instances in a data set are equally beneficial for inducing a model of the data. Some instances (such as outliers or noise) can be detrimental. However, at least initially, the instances in a data set are generally considered equally in machine learning algorithms. Many current approaches for handling noisy and detrimental instances make a binary decision about whether an instance is detrimental or not. In this paper, we 1) extend this paradigm by weighting the instances on a continuous scale and 2) present a methodology for measuring how detrimental an instance may be for inducing a model of the data. We call our method of identifying and weighting detrimental instances reduced detrimental instance learning We examine RIDL on a set of 54 data sets and 5 learning algorithms and compare RIDL with other weighting and filtering approaches. RDIL is especially useful for learning algorithms where every instance can affect the classification boundary and the training instances are considered individually, such as multilayer perceptrons trained with backpropagation (MLPs). Our results also suggest that a more accurate estimate of which instances are detrimental can have a significant positive impact for handling them.\n",
      "================================================\n",
      "Reducing the Effects of Detrimental Instances Not all instances in a data set are equally beneficial for inducing a model of the data. Some instances (such as outliers or noise) can be detrimental. However, at least initially, the instances in a data set are generally considered equally in machine learning algorithms. Many current approaches for handling noisy and detrimental instances make a binary decision about whether an instance is detrimental or not. In this paper, we 1) extend this paradigm by weighting the instances on a continuous scale and 2) present a methodology for measuring how detrimental an instance may be for inducing a model of the data. We call our method of identifying and weighting detrimental instances reduced detrimental instance learning We examine RIDL on a set of 54 data sets and 5 learning algorithms and compare RIDL with other weighting and filtering approaches. reduced detrimental instance learning is especially useful for learning algorithms where every instance can affect the classification boundary and the training instances are considered individually, such as multilayer perceptrons trained with backpropagation (MLPs). Our results also suggest that a more accurate estimate of which instances are detrimental can have a significant positive impact for handling them.\n"
     ]
    }
   ],
   "source": [
    "text_step1 = [word for word in text.split() if not (word.startswith(\"(\") and word.strip(\"().,:;?!/\") in abbreviations)]\n",
    "text_step2 = [abbreviations[word] if word in abbreviations.keys() else word for word in text_step1]\n",
    "print(\" \".join(text_step1))\n",
    "print(\"================================================\")\n",
    "print(\" \".join(text_step2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBSERVATIONS**: Interesting instances of abbreviations can be observed in the above text, which very difficult to handle. The abbreviation definition is RDIL but author has used RIDL twice, one instance of RDIL has been replaced by above preprocessing steps. Another instance is with \"multilayer perceptrons trained with backpropagation (MLPs)\". Here the abbreviation doesnot exactly corresponds to the definition and there are extra words. Also, MLPs contain a lower character at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reducing the Effects of Detrimental Instances Not all instances in a data set are equally beneficial for inducing a model of the data. Some instances (such as outliers or noise) can be detrimental. However, at least initially, the instances in a data set are generally considered equally in machine learning algorithms. Many current approaches for handling noisy and detrimental instances make a binary decision about whether an instance is detrimental or not. In this paper, we 1) extend this paradigm by weighting the instances on a continuous scale and 2) present a methodology for measuring how detrimental an instance may be for inducing a model of the data. We call our method of identifying and weighting detrimental instances reduced detrimental instance learning We examine RIDL on a set of 54 data sets and 5 learning algorithms and compare RIDL with other weighting and filtering approaches. reduced detrimental instance learning is especially useful for learning algorithms where every instance can affect the classification boundary and the training instances are considered individually, such as multilayer perceptrons trained with backpropagation (MLPs). Our results also suggest that a more accurate estimate of which instances are detrimental can have a significant positive impact for handling them.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replace_abbreviations(text):\n",
    "    pattern = r'\\b(([A-Z])\\w*(?:\\s+\\w+)*?)\\s*\\((\\2[A-Z]*)\\)'\n",
    "    abbreviations = {x.group(3):x.group(1) for x in re.finditer(pattern, text, re.I) if contains_abbrev(x.group(3), x.group(1))}\n",
    "    text_step1 = [word for word in text.split() if not (word.startswith(\"(\") and word.strip(\"().,:;?!/\") in abbreviations)]\n",
    "    text_step2 = [abbreviations[word] if word in abbreviations.keys() else word for word in text_step1]\n",
    "    return \" \".join(text_step2)\n",
    "    \n",
    "data_df[\"text\"] = data_df[\"text\"].apply(replace_abbreviations)\n",
    "data_df[\"text\"].values[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reducing the effects of detrimental instances not all instances in a data set are equally beneficial for inducing a model of the data. some instances (such as outliers or noise) can be detrimental. however, at least initially, the instances in a data set are generally considered equally in machine learning algorithms. many current approaches for handling noisy and detrimental instances make a binary decision about whether an instance is detrimental or not. in this paper, we 1) extend this paradigm by weighting the instances on a continuous scale and 2) present a methodology for measuring how detrimental an instance may be for inducing a model of the data. we call our method of identifying and weighting detrimental instances reduced detrimental instance learning we examine ridl on a set of 54 data sets and 5 learning algorithms and compare ridl with other weighting and filtering approaches. reduced detrimental instance learning is especially useful for learning algorithms where every instance can affect the classification boundary and the training instances are considered individually, such as multilayer perceptrons trained with backpropagation (mlps). our results also suggest that a more accurate estimate of which instances are detrimental can have a significant positive impact for handling them.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[\"text\"] = data_df[\"text\"].str.lower()\n",
    "data_df[\"text\"].values[3] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect and Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['www.example1.com', 'www.example2.com']\n",
      "login to  and \n"
     ]
    }
   ],
   "source": [
    "#url\n",
    "text = \"login to www.example1.com and www.example2.com\"\n",
    "pattern = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "\n",
    "urls = re.findall(pattern, text)\n",
    "print(urls)\n",
    "\n",
    "cleaned_text = re.sub(pattern, \"\", text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "data_df[\"text\"] = data_df[\"text\"].apply(\n",
    "    lambda text: re.sub(pattern, \"\", text)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect and Remove Email ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['info@example.com']\n",
      "you can contact us at  \n"
     ]
    }
   ],
   "source": [
    "text = \"you can contact us at info@example.com \"\n",
    "pattern = re.compile(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\")\n",
    "email_ids = re.findall(pattern, text)\n",
    "print(email_ids)\n",
    "cleaned_text = re.sub(pattern, \"\", text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\")\n",
    "data_df[\"text\"] = data_df[\"text\"].apply(\n",
    "    lambda text: re.sub(pattern, \"\", text)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['25-01-2023', '25/01/2023', '25.01.2023']\n",
      "Today's date in different formats is   \n"
     ]
    }
   ],
   "source": [
    "text = \"Today's date in different formats is 25-01-2023 25/01/2023 25.01.2023\"\n",
    "pattern = re.compile(r\"\\d+[\\.\\/-]\\d+[\\.\\/-]\\d+\")\n",
    "dates = re.findall(pattern, text)\n",
    "print(dates)\n",
    "cleaned_text = re.sub(pattern, \"\", text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r\"\\d+[\\.\\/-]\\d+[\\.\\/-]\\d+\")\n",
    "data_df[\"text\"] = data_df[\"text\"].apply(\n",
    "    lambda text: re.sub(pattern, \"\", text)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are not many contractions they have used'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install contractions\n",
    "import contractions\n",
    "contractions.fix(\"There aren't many contractions they've used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reducing the effects of detrimental instances not all instances in a data set are equally beneficial for inducing a model of the data. some instances (such as outliers or noise) can be detrimental. however, at least initially, the instances in a data set are generally considered equally in machine learning algorithms. many current approaches for handling noisy and detrimental instances make a binary decision about whether an instance is detrimental or not. in this paper, we 1) extend this paradigm by weighting the instances on a continuous scale and 2) present a methodology for measuring how detrimental an instance may be for inducing a model of the data. we call our method of identifying and weighting detrimental instances reduced detrimental instance learning we examine ridl on a set of 54 data sets and 5 learning algorithms and compare ridl with other weighting and filtering approaches. reduced detrimental instance learning is especially useful for learning algorithms where every instance can affect the classification boundary and the training instances are considered individually, such as multilayer perceptrons trained with backpropagation (mlps). our results also suggest that a more accurate estimate of which instances are detrimental can have a significant positive impact for handling them.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[\"text\"] = data_df[\"text\"].apply(contractions.fix)\n",
    "data_df[\"text\"].values[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the general abbreviations will also be expanded using contractions fix function. For example gimme: give me, sux : sucks, but they might not be relevant in case of the reasearch articles and is more relevant in case of human communications applications. the dictionary used by contractions library can be obtained from https://github.com/kootenpv/contractions/tree/master/contractions/data    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stopwords-en.txt\", encoding=\"utf-8\") as sw:\n",
    "    STOPWORDS_EN = sw.readlines()\n",
    "    STOPWORDS_EN = [word.replace(\"\\n\", \"\") for word in STOPWORDS_EN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reducing effects detrimental instances instances data set equally beneficial inducing model data. instances (such outliers noise) detrimental. however, initially, instances data set considered equally machine learning algorithms. current approaches handling noisy detrimental instances binary decision instance detrimental not. paper, 1) extend paradigm weighting instances continuous scale 2) methodology measuring detrimental instance inducing model data. method identifying weighting detrimental instances reduced detrimental instance learning examine ridl set 54 data sets 5 learning algorithms compare ridl weighting filtering approaches. reduced detrimental instance learning learning algorithms instance affect classification boundary training instances considered individually, multilayer perceptrons trained backpropagation (mlps). accurate estimate instances detrimental positive impact handling them.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[\"text\"] = data_df[\"text\"].apply(\n",
    "    lambda text: \" \".join([word for word in str(text).split() if word not in STOPWORDS_EN])\n",
    "    )\n",
    "data_df[\"text\"].values[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reducing effects detrimental instances instances data set equally beneficial inducing model data instances such outliers noise detrimental however initially instances data set considered equally machine learning algorithms current approaches handling noisy detrimental instances binary decision instance detrimental not paper 1 extend paradigm weighting instances continuous scale 2 methodology measuring detrimental instance inducing model data method identifying weighting detrimental instances reduced detrimental instance learning examine ridl set 54 data sets 5 learning algorithms compare ridl weighting filtering approaches reduced detrimental instance learning learning algorithms instance affect classification boundary training instances considered individually multilayer perceptrons trained backpropagation mlps accurate estimate instances detrimental positive impact handling them'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[\"text\"] = data_df[\"text\"].apply(\n",
    "    lambda text: re.sub('[%s]' % re.escape(string.punctuation), \"\",text)\n",
    ")\n",
    "data_df[\"text\"].values[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reducing effects detrimental instances instances data set equally beneficial inducing model data instances such outliers noise detrimental however initially instances data set considered equally machine learning algorithms current approaches handling noisy detrimental instances binary decision instance detrimental not paper  extend paradigm weighting instances continuous scale  methodology measuring detrimental instance inducing model data method identifying weighting detrimental instances reduced detrimental instance learning examine ridl set  data sets  learning algorithms compare ridl weighting filtering approaches reduced detrimental instance learning learning algorithms instance affect classification boundary training instances considered individually multilayer perceptrons trained backpropagation mlps accurate estimate instances detrimental positive impact handling them'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[\"text\"] = data_df[\"text\"].apply(\n",
    "    lambda text: re.sub('\\d','',text)\n",
    ")\n",
    "data_df[\"text\"].values[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Extra Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reducing effects detrimental instances instances data set equally beneficial inducing model data instances such outliers noise detrimental however initially instances data set considered equally machine learning algorithms current approaches handling noisy detrimental instances binary decision instance detrimental not paper extend paradigm weighting instances continuous scale methodology measuring detrimental instance inducing model data method identifying weighting detrimental instances reduced detrimental instance learning examine ridl set data sets learning algorithms compare ridl weighting filtering approaches reduced detrimental instance learning learning algorithms instance affect classification boundary training instances considered individually multilayer perceptrons trained backpropagation mlps accurate estimate instances detrimental positive impact handling them'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[\"text\"] = data_df[\"text\"].apply(\n",
    "    lambda text: re.sub(\" +\", \" \", text).strip()\n",
    ")\n",
    "data_df[\"text\"].values[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spelling correction using TextBlob\n",
    "# !pip install -U textblob\n",
    "# !python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reducing effects detrimental instances instances data set equally beneficial inducing model data instances such outlines noise detrimental however initially instances data set considered equally machine learning algorithms current approaches handling noisy detrimental instances binary decision instance detrimental not paper extend paradise weighing instances continuous scale methodology measuring detrimental instance inducing model data method identifying weighing detrimental instances reduced detrimental instance learning examine ride set data sets learning algorithms compare ride weighing faltering approaches reduced detrimental instance learning learning algorithms instance affect classification boundary training instances considered individually multilayer perceptions trained backpropagation maps accurate estimate instances detrimental positive impact handling them'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "data_df[\"text\"] = data_df[\"text\"].apply(\n",
    "    lambda text: str(TextBlob(text).correct())\n",
    ")\n",
    "data_df[\"text\"].values[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This option might not be feasible for large datasets as it took 38 minutes to run for small dataset with 448 records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion to British or American English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on https://stackoverflow.com/questions/42329766/python-nlp-british-english-vs-american-english\n",
    "\n",
    "import requests\n",
    "\n",
    "# def americanize(string):\n",
    "#     url =\"https://raw.githubusercontent.com/hyperreality/American-British-English-Translator/master/data/british_spellings.json\"\n",
    "#     british_to_american_dict = requests.get(url).json()    \n",
    "\n",
    "#     for british_spelling, american_spelling in british_to_american_dict.items():\n",
    "#         string = string.replace(british_spelling, american_spelling)\n",
    "  \n",
    "#     return string\n",
    "\n",
    "import requests\n",
    "\n",
    "def britishize(string):\n",
    "    url =\"https://raw.githubusercontent.com/hyperreality/American-British-English-Translator/master/data/american_spellings.json\"\n",
    "    american_to_british_dict = requests.get(url).json()    \n",
    "\n",
    "    for american_spelling, british_spelling in american_to_british_dict.items():\n",
    "        string = string.replace(american_spelling, british_spelling)\n",
    "  \n",
    "    return string\n",
    "\n",
    "import requests\n",
    "import re\n",
    "def americanize(string):\n",
    "    url =\"https://raw.githubusercontent.com/hyperreality/American-British-English-Translator/master/data/british_spellings.json\"\n",
    "    british_to_american = requests.get(url).json()    \n",
    "    for british_spelling, american_spelling in british_to_american.items():\n",
    "        #string = string.replace(british_spelling, american_spelling) \n",
    "        string = re.sub(f'(?<![a-zA-Z]){british_spelling}(?![a-z-Z])', american_spelling, string)\n",
    "        string = re.sub(f'(?<![a-zA-Z]){british_spelling}(?![a-z-Z])', american_spelling, string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Discount analyze'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Discount analyse\"\n",
    "americanize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b692d53c618c7279fd8f739b29cb9b56968a589a1952adc40c54d9e83ebc1323"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('openai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
